{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapting to Your Use Case\n",
    "\n",
    "To fully leverage the power of QB-RAG, you can prompt engineer the two prompts which are responsible for: question generation and answerability.\n",
    "\n",
    "1. Customizing Prompts:\n",
    "To modify the prompts, edit the converters.py file. You'll find two Prompt objects named `QUESTION_GENERATION_PROMPT` and `ANSWERABILITY_PROMPT`. Adjust the role/task and one/few-shot examples within these prompts to better fit your specific use case.\n",
    "\n",
    "2. Controlling Question Generation:\n",
    "You can also influence the number of questions generated from each chunk by implementing custom logic in the `_create_question_generation_prompt()` function. Currently, the default is set to generate 10 questions per chunk. Modify this logic to align with your requirements.\n",
    "\n",
    "By tailoring these aspects, you can optimize the technique to better suit your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Secret Keys\n",
    "\n",
    "Please ensure you've followed the instructions in the README to set up your `.env` file. If you prefer to use API keys directly rather than through a `.env` file, you'll need to incorporate them into your code manually as per your usual practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the LLM\n",
    "\n",
    "In this implementation, I have utilized OpenAI's GPT-3.5 Turbo as the language model (LLM). However, you are welcome to use any LLM of your choice, provided it is supported by LangChain or inherits from the `langchain_core.language_models.llms` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Vector Database\n",
    "\n",
    "In this project, Pinecone is used as the vector database. However, you can choose any vector database that is a there in Langchain (subclass of `langchain_core.vectorstores.base`).\n",
    "\n",
    "Additionally, you have the flexibility to select the embedder for embedding questions. In this setup, I have utilized OpenAI's `text-embedding-3-small`. Adjust the embedder according to your requirements as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"qb-rag-index\"  # change if desired\n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "vector_store = PineconeVectorStore(index=index, embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "Prepare a list of all the context/document text (LangChain lingo) that you have stored after the chunking process of your data source.\n",
    "\n",
    "In this implementation, I am loading the chunks from a `text.file`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunks_to_list(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        file_content = file.read().strip()\n",
    "        chunks = file_content.split(\"\\n\\n\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "file_path = \"text.txt\"\n",
    "chunks = read_chunks_to_list(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Converter Object and Loop\n",
    "\n",
    "- **`questions_generated`**: This argument specifies how many questions will be generated from each context chunk or document text.\n",
    "- **`max_retries`**: This argument sets the number of retry attempts in case of failure, such as when the output or response from the LLM does not match the expected format.\n",
    "- **`reproducibility`**: This argument determines how many times the answerability of the generated questions will be tested. The results are aggregated using a voting mechanism to determine the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from QB_RAG.converter import Converter\n",
    "\n",
    "cvt = Converter(vector_store, llm, questions_generated=10, max_retries=1, reproducibility=1)\n",
    "\n",
    "for i in chunks:\n",
    "    cvt.add_documents(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
