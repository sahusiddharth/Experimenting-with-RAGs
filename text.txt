Abstractive summarization is the task of generating concise summaries that capture the key ideas in a source document. Unlike extractive summarization, which lifts entire sentences from the original text, abstractive summarization involves rephrasing and condensing information to create a newer, shorter version. This process requires a deep understanding of the content, the ability to identify the most important points, and a careful approach to avoid introducing hallucination defects. To evaluate abstractive summaries, Kryscinski et al. (2019) proposed four key dimensions: fluency, coherence, consistency, and relevance. Fluency asks whether sentences in the summary are well-formed and easy to read, while coherence examines whether the summary as a whole makes sense and is logically organized. Consistency checks whether the summary accurately reflects the content of the source document, ensuring no new or contradictory information is added. Lastly, relevance evaluates whether the summary focuses on the most important aspects of the source document, including key points and excluding less relevant details.

As modern language models have improved, generating grammatically correct and readable sentences has become less of a concern, making fluency a lower priority in evaluation. Similarly, coherence is becoming less of an issue, particularly for shorter summaries consisting of just a few sentences. This shift in focus leaves factual consistency and relevance as the primary evaluation concerns, which can be effectively framed as binary classification tasks. Despite the widespread use of n-gram-based methods (like ROUGE and METEOR), similarity-based evaluations (like BERTScore and MoverScore), and LLM-based evaluations (such as G-Eval), these approaches have proven to be unreliable or impractical in many cases. They often require gold reference summaries, which can become a bottleneck due to the need for collecting these references, training annotators, and continuously auditing for quality. Furthermore, studies have shown that generated summaries can sometimes surpass the quality of reference summaries, as observed in datasets like CNN/DailyMail and XSUM. Additionally, the metrics’ poor separation of distributions can lead to high variance from the ground truth, making these methods less effective for evaluating summarization tasks.

To address the challenge of measuring factual consistency, one approach is to finetune a natural language inference (NLI) model as a learned metric. The NLI task involves predicting whether a hypothesis logically follows from, is neutral to, or contradicts a given premise. By treating the source document as the premise and the generated summary as the hypothesis, NLI models can effectively evaluate the factual consistency of summaries. If the summary contradicts the source document, it indicates factual inconsistency, commonly referred to as a hallucination. Typically, NLI models return probabilities for entailment, neutral, and contradiction dimensions. To assess factual inconsistency, the neutral dimension can be dropped, and a softmax can be applied to the remaining entailment and contradiction dimensions, with the probability of contradiction serving as the key metric. It’s crucial to confirm which dimension represents entailment in the NLI model being used, as this can vary between models—for instance, Google’s T5 NLI model places entailment at dimension 1, while Meta’s BART NLI model places it at dimension 2.

With just a few hundred task-specific samples, an NLI model can begin to identify obvious factual inconsistencies and likely outperform n-gram-based, similarity-based, and LLM-based evaluations. When trained with a thousand samples or more, the NLI model can become a robust tool for evaluating factual consistency and may even serve as a guardrail against hallucinations in generated summaries. To minimize the need for extensive data annotation, it is possible to bootstrap the model using open-source, permissive-use datasets like the Factual Inconsistency Benchmark (FIB) and the Unified Summarization Benchmark (USB). Performance graphs of NLI-based evaluations for factual inconsistency on the FIB dataset show significant improvement after finetuning on USB and FIB. While there is still room for further enhancement, these results demonstrate that even minimal finetuning on open-source data can substantially improve performance, with ROC-AUC increasing from 0.56 (which is nearly random) to 0.85, indicating the potential of NLI models in this context.
